---
title: 'Topic Models: A tool for uncovering hidden themes in data'
author: 'Mohit Yadav | Quant UX Conference 2023'

output:
    github_document:
        toc: true
        number_sections: true
---


```{r  , include = FALSE}
#update.packages(c('rmarkdown','quanteda','quanteda.textstats','stopwords','topicmodels','topicdoc','LDAvis'))
```


# Loading dataset

Loading **20 newsgroup dataset** [via scikit](./Modules/dataLoader.py)

The dataset has been [pre-processed](./Modules/lemmatiser.py) as following: 

+ Removed stopwords
+ Lemmatised
+ Removed docs with less than 20 to and more than 10000 tokens

```{r ReadingData}
df <- read.csv('./Data/lemmatised_text.csv')
dim(df)

```

Splitting dataset into **train and held out(test)** sets for [perplexity](# Perplexity) evaluation measure.
```{r}
train_doc_n = 0.85 * dim(df)[1] #85-15 split into train and test
train_data <- df[0:(round(train_doc_n)-1),] 
test_data <- df[round(train_doc_n):dim(df)[1],]
temp_dat <- df[0:1000,]
```


# Pre-processing
```{r LoadingLibraries, include= FALSE}
library('quanteda')
library('quanteda.textstats')
library('stopwords')
```

**Tokenising via quanteda**

+ Removing punctuations
+ Removing numbers
+ Removing words with less than 3 characters
+ Removing stopwords
+ Removing regex pattern of dashes (--+)
+ Identifying frequent multiword tokens (colloacation)
+ Creating single token from multiword tokens

**Pre-processing Document Feature Matrix**

+ Converting documents to Document Feature Matrix (dfm_matrix)
+ Removing uncommon terms (less than 25 document occurences)
+ Removing common terms (more than 7% document occurences)
+ Removing docs and rows that have 0 tokens after pre-processing

## Pre-processing training data
```{r Tokenising, cache=TRUE}
# Tokenising
source('./Modules/R/rTokenising.r')
toks <- preProcess(train_data)
```

```{r}
# Converting documents to Document Feature Matrix (dfm_matrix)
dfm_matrix <- dfm(toks)
dim(dfm_matrix)
```


```{r}
# Removing uncommon terms (less than 25 document occurences)
# Removing common terms (more than 7% document occurences)
dfm_matrix <- dfm_trim(dfm_matrix, min_termfreq = 25, max_termfreq = round(length(train_data) * 0.07))
dim(dfm_matrix)
```

```{r}
# Removing docs and rows that have 0 tokens after pre-processing
source('./Modules/R/zeroRows.r')

ZeroRowDfm <- dfm_matrix[rowSums(dfm_matrix) == 0,]
dim(ZeroRowDfm)

# DFM with removal of rows with 0 terms after preprocessing.
dfm_matrix <- dfm_matrix[rowSums(dfm_matrix) != 0,]
dim(dfm_matrix)

# Dataset with removal of docs with 0 terms after preprocessing.
zeroIndex <- zeroRowIndex(ZeroRowDfm)
train_data_processed <- train_data[-c(zeroIndex)]
```


## Pre-processing held out data

Repeating the above **[pre-processing steps](# Pre-processing)** for testing data.
```{r}
# Tokenising
toks_test <- preProcess(test_data)

# Converting documents to Document Feature Matrix (dfm_matrix)
dfm_matrix_test <- dfm(toks_test)
dim(dfm_matrix_test)

# Removing uncommon terms (less than 15 document occurences)
# Removing common terms (more than 7% document occurences)
dfm_matrix_test <- dfm_trim(dfm_matrix_test, min_termfreq = 15, max_termfreq = round(length(temp_dat) * 0.07))
dim(dfm_matrix_test)
```

```{r}
# Removing docs and rows that have 0 tokens after pre-processing
ZeroRowDfm_test <- dfm_matrix_test[rowSums(dfm_matrix_test) == 0,]

# DFM with removal of rows with 0 rows after preprocessing.
dfm_matrix_test <- dfm_matrix_test[rowSums(dfm_matrix_test) != 0,]
dim(dfm_matrix_test)

# Dataset with removal of docs with 0 rows after preprocessing.
zeroIndex_test <- zeroRowIndex(ZeroRowDfm_test)
test_data_processed <- test_data[-c(zeroIndex_test)]

```

## Converting dfm to topicmodels format
```{r ConvertingDFMType}
# Converting dfm_matrix to topicmodels format.
dfm_matrix_tm <- convert(dfm_matrix, to = 'topicmodels')
dfm_matrix_tm_test <- convert(dfm_matrix_test, to = 'topicmodels')
```

# Modelling
```{r , include = FALSE}
library('topicmodels')
library('topicdoc')
```

**LDA Model Parameters**

+ k : Number of topics
+ alpha : Document distribution over topics

Topics get more fine grained with increased k parameter (number of topics).


+ **Modelling 5 topic (alpha = 0.08) model.**
```{r Model 5 ,results='hide', cache=TRUE}

model_lda_5 <- LDA(dfm_matrix_tm, 
                    k = 5, method = 'Gibbs', 
                    control = list(alpha = 0.08, burnin = 500 , verbose = 100 , seed = 1234))

saveRDS(model_lda_5,'./Models/model_lda_5.rds')
model_lda_5 <- readRDS('./Models/model_lda_5.rds')
```

```{r}
terms(model_lda_5, 5)
```

+ **Modelling 21 topic (alpha = 0.08) model.**
```{r Model 21, results='hide', cache=TRUE}
model_lda_21 <- LDA(dfm_matrix_tm, 
                    k = 21, method = 'Gibbs', 
                    control = list(alpha = 0.08, burnin = 500 , verbose = 100 , seed = 1234))

saveRDS(model_lda_21,'./Models/model_lda_21.rds')
model_lda_21 <- readRDS('./Models/model_lda_21.rds')
```


+ **Modelling 45 topic (alpha = 0.08) model.**
```{r Model 45, results='hide', cache=TRUE}
model_lda_45 <- LDA(dfm_matrix_tm, 
                    k = 45, method = 'Gibbs', 
                    control = list(alpha = 0.08, burnin = 500 , verbose = 100 , seed = 1234))

saveRDS(model_lda_45,'./Models/model_lda_45.rds')
model_lda_45 <- readRDS('./Models/model_lda_45.rds')
```


+ **Modelling 120 topic (alpha = 0.1) model.**
```{r Model 120, results='hide', cache=TRUE, eval=FALSE}

model_lda_120 <- LDA(dfm_matrix_tm, 
                    k = 120, method = 'Gibbs', 
                    control = list(alpha = 0.1, burnin = 500 , verbose = 100 , seed = 1234))

saveRDS(model_lda_120,'./Models/model_lda_120.rds')
model_lda_120 <- readRDS('./Models/model_lda_120.rds')
```

+ **Modelling 21 topic (alpha = 1.2) model.**
```{r , results='hide', cache=TRUE}
model_lda_21_a12 <- LDA(dfm_matrix_tm, 
                    k = 21, method = 'Gibbs', 
                    control = list(alpha = 1.2, burnin = 500 , verbose = 100 , seed = 1234))

saveRDS(model_lda_21_a12,'./Models/model_lda_21_a12.rds')
model_lda_21_a12 <- readRDS('./Models/model_lda_21_a12.rds')
```

# Model Evaluation

Evaluating topic models via two approaches:

- [Human Evaluation](#Human Evaluation)
- [Automated Evaluation](#Automated Evaluation)


## Human Evaluation

### Word Intrusion
**Words intruding other words estimated for the topic.**
```{r}
Topic6 <- terms(model_lda_21,10)[51:60]
writeLines(Topic6)
```
In the above example word **sale** and **offer** seems to be intruding the other words in the topic.
```{r , include = FALSE}
Topic2Names <- c('1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','21')
```

### Topic Intrusion
**Topic intruding other topics estimated for the document.**
```{r}
TopicsInDoc <- topics(model_lda_21,3,0.1)

docnum = 12321

writeLines(train_data_processed[docnum])
TopicsInDoc[docnum]
```

```{r , include=FALSE}
Topic2 <- terms(model_lda_21,10)[11:15]
Topic3 <- terms(model_lda_21,10)[21:25]
Topic5 <- terms(model_lda_21,10)[41:45]
docTopics <- cbind(Topic2,Topic3,Topic5)
```
```{r}
docTopics
```

In the above example word **Topic2** seems to be intruding the **Topic3** and **Topic5** for the document.

## Automated Evaluation

These metrics are often useful for **model selection** amongst different models.

### Perplexity

**Probability (Held_Out Documents | Model estimated via Train Documents)**

Note: Depending on the package the perplexity measure could have different implementation and hence the value does not exactly translate to probability scale (0-1 or 0-100). Please read the documentation of the package you are using. Here **lower** the value; better.

```{r , results='hide'}
perplexity_21 <- perplexity(model_lda_21, dfm_matrix_tm_test)
perplexity_21_a12 <- perplexity(model_lda_21_a12, dfm_matrix_tm_test)
```

```{r}
perplexity_21
perplexity_21_a12
```

### Coherence

**Top words in a topic tend to co-occur in document**

Some packages have different implementations of coherence measures availaible (Cv, Cuci, Cnpmi, etc.). 

Note: Here **higher** the value (towards 0); better.

```{r}
# Coherence measure for each Topic
topic_diagnostics(model_lda_5,dfm_matrix_tm)$topic_coherence

# Mean coherence measure for the model
mean(topic_diagnostics(model_lda_5,dfm_matrix_tm)$topic_coherence)
```

# Model Visualisation

LDAvis package helps visualise a topic model. 

This could help in providing an overview of the **words in different topics** and the **size of the topic** in the corpus.

```{r , include= FALSE}
#For visualisation
library('LDAvis')
```

```{r , eval=FALSE}
# Creating JSON object for LDAvis and visualisation
source('./Modules/R/CreateJsonObj.r')
json <- jsonPrep(model_lda_21,dfm_matrix)
serVis(json)
```
![LDA Vis](./Fig/Visualisation.png)


# Document similarity

Finding documents with **small distance** between their **distribution over topics**.

```{r}
similarity_matrix <- distHellinger(model_lda_5@gamma[1:5,])
print(similarity_matrix)

```

Distance between distribution of **Document 1** and **Document 5** is small **(0.002)**. They also seem similar.

```{r}

writeLines(train_data_processed[1])

writeLines(train_data_processed[5])
```

**Happy Topic Modelling**
